#File : llm_models 
    #Overall explanation : 
        #Here we declare the interfaces that will be used in other repos of this code 
        #For example, if we want to use some openai sdk we instiantiate the corresponding object from this file and set it we the right model (eg gpt-4)

from typing import Dict, List, Tuple
from chat_bot_session_ui.src.utils.custom_logger import Logger
from chat_bot_session_ui.src.utils.tokens_utils import TokenCounter 
from .openai_models import GPTCall



#This object will contain all the details related to setting the llm model
class LLMDataLoader :
    def __init__(self,model:str, api_key:str,llm_spec:Dict, modal_type:str="text-to-text") : 
        """
            model : str = the gpt model you want to use ,  
            api_key : str = your openai api key , 
            modal_type : str = to precise the type of input/output that will be used in the LLMCall (for now it handles only text-to-text)
            llm_spec : str = it's a dictionary that contain parameters to set the llm 
            It can contain the following key-value : 
                top_p : float = a probability that controls the diversity of the output using nucleus sampling 
                n : int = number of response that will be generated by the model , 
                stream : bool = for streamed response 
                max_tokens : int = the maximum of tokens generated in the reponse 
                user : str = unique identifier for end user,
                presence_penalty : float = encourages the model to use different tokens 
                logit_bias : Dict[int,float] = adjusts probabilty of specific tokens value between -2 and 2 
        """
        self.model = model 
        self.api_key = api_key
        self.modal_type = modal_type 
        self.llm_spec = llm_spec



#This class that will be instantiated in agent pattern 
#It should be able to handle the different objects of specific LLm provider (eg gpt, clauder)
#Or even open source LLM deployed by user 
class LLMCall : 
    def __init__(self  ,llm_data_loader:LLMDataLoader, logger:Logger=None) :
        
        """
                api_key : str = your openai api key , 
                modal_type : str = to precise the type of input/output that will be used in the LLMCall (for now it handles only text-to-text)
                logger : Logger  = object to keep try of the calls 
        """
        self.llm_data_loader = llm_data_loader 
        self.logger = logger 
        self.token_counter = TokenCounter(model=self.llm_data_loader.model,modal_type=self.llm_data_loader.modal_type,logger=self.logger)
        if self.llm_data_loader.modal_type == "text-to-text" : 
            if self.llm_data_loader.model.startswith("gpt") : 
                self.model_specs = {"api_key" : self.llm_data_loader.api_key, 
                                    "model" : self.llm_data_loader.model, 
                                    **self.llm_data_loader.llm_spec}
                self.model_client = GPTCall(model_specs=self.model_specs,
                                                logger = logger )
                print("GPTCall created correctly ")
                

    def __call__(self,messages:List[Dict]) : 
        return self.model_client(messages)

    def is_streaming_behaviour_set(self) -> bool : 
        if "stream" in self.model_specs : 
            return True 
        else : 
            return False

    
        



